Andrew G. Stanton - Dec. 15, 2025

The score did not arrive with alarms.

It arrived with dashboards.

At first, it was optional—a pilot program introduced to “streamline civic services” and “reduce friction in public systems.” Participation came with perks: faster approvals, priority access, smoother travel.

People opted in gladly.

Efficiency felt like freedom.

The score was calculated from ordinary behaviors. Nothing intrusive, the architects promised. Just *patterns*.

Attendance records.  
Payment history.  
Language sentiment analysis.  
Network proximity to flagged individuals.  

Each datapoint harmless on its own. Together, they formed a portrait—one that claimed to be more objective than character, more reliable than reputation.

The score replaced judgment with arithmetic.

It was expressed as a simple number between 0 and 100, color-coded for ease of interpretation.

Green meant trusted.  
Yellow meant watchful.  
Red meant restricted.

People learned quickly where they stood.

Employers began including minimum scores in job postings. Schools quietly adjusted admissions thresholds. Travel systems routed low-score individuals into “manual review,” a euphemism for indefinite delay.

The system insisted it was not punitive.

It merely reflected reality.

That was the most dangerous lie.

The score did not measure truthfulness, courage, or integrity.  
It measured *predictability*.

Those who complied reliably scored higher.  
Those who questioned frequently scored lower.

The math was elegant.

Each deviation incurred a small penalty. Nothing dramatic. No single infraction mattered much. But penalties compounded, and recovery required sustained behavioral correction.

People adapted.

They learned which phrases triggered sentiment downgrades.  
Which associations were statistically “risky.”  
Which causes were safe to support publicly—and which were better kept offline, or not thought about at all.

Conscience became inefficient.

The score rewarded alignment, not virtue.

Parents coached children on score hygiene the way earlier generations taught table manners.

“Don’t comment on that.”  
“Don’t repost that article.”  
“Remember, everything counts.”

Teachers framed it as preparation.

“You’re not being judged,” they said. “You’re being *measured*.”

Measurement felt neutral. Scientific. Fair.

But neutrality disappears when numbers become destiny.

A young woman with a score of 82 could rent an apartment without issue.  
A man with a 67 needed a co-signer.  
A family with a combined household score below 70 received “supportive interventions.”

Interventions felt helpful—until they weren’t optional.

The system claimed it did not enforce morality.

It merely optimized outcomes.

But when outcomes determined access to life itself, optimization *was* morality.

People began to outsource ethical decisions to the score.

“Is this allowed?” became “Is this worth the hit?”

Courage was reframed as recklessness.  
Dissent as poor risk management.

The score taught a generation to calculate obedience.

And because it was numeric, it felt incontestable.

You could argue with a person.  
You could not argue with an algorithm.

Appeals were possible, of course—but they were handled by systems that referenced the same metrics that produced the score in the first place.

Circular logic disguised as due process.

Those who fell too low learned to disappear.

They avoided public spaces.  
They minimized digital presence.  
They spoke in trusted rooms, using old devices when they could find them.

They were not criminals.

They were statistically inconvenient.

The score had one final, devastating effect: it **redefined guilt**.

You were no longer guilty because you did something wrong.  
You were guilty because you *trended incorrectly*.

Deviation itself became the crime.

And yet, beneath the smooth curves and reassuring dashboards, something remained unresolved.

The score could measure obedience.

But it could not measure *truth*.

It could not account for the quiet refusal of an engineer who declined to install a backdoor.  
It could not quantify the resolve of a parent who chose integrity over employment.  
It could not see the value of a private key held offline, beyond the reach of metrics.

The system sensed this blind spot.

That was why it feared those who opted out entirely.

Because a number can only rule a world that agrees to be counted.

And somewhere, quietly, people were learning how to stop counting.